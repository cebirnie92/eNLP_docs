{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNLP Pipeline\n============\nThe following example takes a raw extract of norwegian texts, removes stopwords and lemmatizes prior to analysing the\ndistribution of the words in the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nplt.close('all') # very important for read the docs to avoid it crashing due to memory\n\nfrom enlp.processing.stdtools import get_stopwords, tokenise\nfrom enlp.pipeline import NLPPipeline\nfrom enlp.visualisation.freq_distribution import wordcloud_plot\n\nimport spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Spacy's Norwegian language model and the example text\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "langmodel = spacy.load('nb_dep_ud_sm')\nwith open(\"example_data/no_den_stygge_andungen.txt\", \"r\") as file:\n    text = file.read()\ntext = text.replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get a list of stopwords to be removed from the text\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get stopwords\nall_stopwords, stopwords_nb, stopwords_en = get_stopwords()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using NLP pipeline class to create a processing workflow. The pipeline shall involve:\n- remove punctuation\n- remove stopwords\n- stem remaining words\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Initialise object\nprocessed_text = NLPPipeline(langmodel, text)\n\n# Run processing as a pipeline\nprocessed_text.rm_punctuation().rm_stopwords(stopwords=all_stopwords).nltk_stem_no()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare text strings of firs 80 characters of the original and processed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print ('Original: ', text[:80], '...')\nprint ('Processed: ', processed_text.text[:80], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wordcloud comparison between most common words\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nwordcloud_plot(tokenise(langmodel,text), ax=ax1)\nwordcloud_plot(processed_text.tokenise().tokens, ax=ax2)\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}